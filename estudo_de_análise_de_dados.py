# -*- coding: utf-8 -*-
"""Estudo de análise de dados.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTFBwwko1QCKgd91x8fle13dDYUXc3sn
"""

import pandas as pd

dfClientes = pd.read_excel('caso_estudo.xlsx', sheet_name = 'clientes') ##df = DataFrame, ele transforma os dados tem tabelas
dfLojas = pd.read_excel('caso_estudo.xlsx', sheet_name = 'lojas')
dfProdutos = pd.read_excel('caso_estudo.xlsx', sheet_name = 'produtos')
dfVendas = pd.read_excel('caso_estudo.xlsx', sheet_name = 'vendas')
dfPag = pd.read_excel('caso_estudo.xlsx', sheet_name = 'pagamentos')

"""# Analise Preliminar"""

dfClientes.sample(5) ## pode ser usado no lugar do sample(simplifica os dados na quantidade que você quiser) o head para os primeiros, ou tail para os ultimos

dfClientes.isna().sum() ## melhor forma de saber a quantidade de dados nulos em um conjunto de dados

dfClientes[dfClientes.isna().T.any()] ##os [] é a forma de criar um filtro, e precisa do Transpostas "T" e o Any para que funcione da forma correta

dfClientes.sexo.unique() ##aqui o unique busca os caracteres unicos desta coluna, o que também é uma forma de localizar dados nulos ou incorretos, antes de iniciar a limpeza dos dados.

dfProdutos

dfProdutos.boxplot(column = ['valor']) ##o boxplot é uma ferramenta gráfica utilizada para ilustrar um conjunto de dados.

"""O Box Plot, também chamado diagrama de caixa, é uma ferramenta gráfica utilizada para ilustrar um conjunto de dados. Por meio dele, é possível visualizar a distribuição de dados com base em cinco estatísticas:

    o mínimo;

    o primeiro quartil (Q1);

    a mediana;

    o terceiro quartil (Q3);

    o máximo.

Esses valores também são conhecidos como resumo dos cinco números.

Ele também pode informar os valores discrepantes (outliers) dos dados, oferecendo uma medida complementar para o desenvolvimento de perspectivas sobre as informações passadas através dos dados.

Além disso, o Box Plot pode informar a posição dos dados, sua simetria, dispersão, cumprimento da cauda de distribuição e se estão ou não distorcidos.

No gráfico do Box Plot, a haste vertical é interpretada de baixo para cima, sendo que a parte inferior indica o mínimo e a superior indica o máximo, sempre desconsiderando possíveis outliers.

O retângulo central da haste possui três linhas que estão na horizontal: a linha de baixo, representada pelo contorno externo inferior do retângulo, indica o primeiro quartil. A linha de cima, que é o contorno externo superior do retângulo, indica o terceiro quartil. Já a linha interna indica a mediana ou o chamado segundo quartil.

Valores discrepantes, outliers e extremos são representados por asteriscos ou pontos, e indicam pontos atípicos no gráfico.
"""

dfProdutos[dfProdutos.valor < 3000000].boxplot(column = ['valor']) ## aqui foi aplicado um filtro no boxplot eliminando o valor de 3M que estava no outlier. Importante ressaltar que mesmo existindo um outlier aqui, é necessario avaliar se ele faz sentido neste contexto.

"""Após identificar o valor problematico, é importante entender o motivo dele estar lá, procurando em outras bases de dados, ou falando com uma pessoa que possa saber o motivo da discrepância. Nesse caso podemos analisar a planilha de vendas."""

dfProdutos[dfProdutos.valor > 3000000]

dfVendas[dfVendas.id_produto == 10].count() ##aqui novamente estamos fazendo um filtro para identificar quantas vezes o produto discrepante foi vendido atravez do id do produto, e tentar achar o valor correto, e o item correto.

dfVendas.describe() ##Describe pode ser utilizado para descrever o tipo do dado, e também pode ser utilizado para entender a quantidade maxima e minima daquele dado.

"""Ex: podemos saber que a quantidade maxima de vendas foi de 3000 e o minimo de 1, também é posivel identificar a quantidade maxima e minima de lojas, produtos e clientes.

# Data Cleanning

Após identificar as discrepância, é a hora de limpar os dados.

Nesta etapa vamos:


*   Lidar com os dados nulos.
*   Buscar outliers e tratativas.
*   Buscar incosistencias entre bases de dados.
*   Buscar dados duplicados.
*   Tratar dados como as datas.
*   Identificar chaves primárias (Dado único que será referenciado em outras bases de dados).

## Dados nulos

utilizar o ***isnull()*** para localizar os dados nulos
"""

dfClientes.loc[dfClientes.nome.isnull(),'nome'] = 'Sem Nome' ##o comando loc é utilizado de forma similar a uma busca em um array ou em um objeto json [index]
dfClientes.loc[dfClientes.sexo.isnull(),'sexo'] = 'O' ##Outros
dfClientes.loc[dfClientes.dt_nasc.isnull(),'dt_nasc'] = '1/1/2024' ## uma dica boa é utilizar uma data de nascimento em que sabemos que é impossivel a pessoa ter realizado a compra, neste caso a pessoa que realizou a compra teria menos de 1 ano, na data em que fiz esta analize

"""Este trecho de código esta utilizando o filtro para substituir o dado nulo para um dado que seja possivel localizar depois"""

dfClientes.loc[[269,287], :] ## utilizando o loc para procurar no index clientes especificos

"""Aqui temos um exemplo de como ficaria a tabela após aplicar as correçoes com os filtros"""

dfClientes.isnull().sum() ## utilizando o isnull e o sum para ver a soma de dados nulos em um conjunto de dados

"""Confirmando que não há mais dados nulos na tabela

## Outliers

Outliers são apenas dados númericos, que são localizados atravez do comando boxplot().
"""

dfProdutos

"""é importante ressaltar que não localizando o valor real do produto dentro dos dados disponiveis, procurar uma pessoa que saiba qual é o nome do produto e o seu real valor."""

dfProdutos.loc[9, 'valor'] = dfProdutos.valor[9]/10000 ## Aqui estou substituindo o valor do produto pelo valor que o "analista que tem mais conhecimento" me passou.

dfProdutos.boxplot(column=['valor']) ## consultando novamente o boxplot para verificar se esta tudo normalizado.

"""##Consistência

Garantir que as tabelas conseguem conversar uma com a outra, garantindo que não tenha nenhum dado "ruim" ou discrepântes

Utilizaremos muito o ***isin()***
"""

dfVendas.id_cliente.isin(dfClientes.id).any() ## é possivel utilizar o "any()" para verificar se os dados estão iguais

~dfVendas.id_cliente.isin(dfClientes.id).any() ## também é possivel fazer o oposto utilizando o simbolo de not (~)

"""Lembrando que o ***any()*** devolve o valores booleanos."""

dfVendas[~dfVendas.id_cliente.isin(dfClientes.id)] ##também é possivel aplicar um filtro para vizualizar a tabela.

dfVendas[~dfVendas.id_loja.isin(dfLojas.id)]

dfVendas[~dfVendas.id_produto.isin(dfProdutos.id)].count() ##também é possivel utilizar o "count" para verificar a contagem de erros. Lembrando que o "count()" trás valores de "0" para falso e "1" para verdadeiro. Caso tivesse algum valor discrepante, ele iria trazer o resultado da soma dos verdadeiros.

"""Aqui basicamente estamos cruzando os dados de id entre a tabela de vendas e as demais para localizar erros.

Nesse caso como não tem nenhum erro, logo ele não vai localizar nenhuma discrepância.
"""

dfPag[~dfPag.id_venda.isin(dfVendas.id)] ##Verificando se existe algum id de pagamento que não está na tabela de vendas

dfVendas[~dfVendas.id.isin(dfPag.id_venda)].count() ##Verificando quantas vendas não tiveram pagamentos

"""##Dados Duplicados

Verificando os dados duplicados.

Aqui utilizaremos principalmente a função duplicated()
"""

dfClientes[dfClientes.nome.duplicated()] ##Verificando dados duplicados pelo nome do cliente

dfClientes[dfClientes.nome == 'Anna Melo'] ##Verificando se as colunas estão identicas

dfClientes.duplicated().sum() ##Fazendo a soma do total de duplicatas com o id

dfClientes.drop('id',axis=1).duplicated().sum() ##Verificando a soma do total de duplicatas sem o id, porque o id pode "sujar" a analise pois o cadastro pode ter sido realizado pela mesma pessoa em momentos diferentes. Gerando ids de clientes diferentes

"""o comando ***drop()*** elimina uma coluna ou linha selecionada.

Sua sintaxe é *drop*(**'nome da coluna ou linha'**, *axis* = (*numero que representa linha ou coluna*.(**0 para linha**, **1 para coluna**)

Caso os dados do cliente esteja duplicado, é interessante agrupar os dados dele, para que se possa ter por exemplo uma noção de quanto o cliente realmente gastou com a loja.
"""

dfVendas[dfVendas.drop('id', axis=1).duplicated()] ##filtrando quantas vendas repetidas foram realizadas, sem a coluna id.

"""Quando se utiliza o filtro desta forma ele retorna apenas o ultimo registro duplicado.

Para se ter uma noção do total é interessante utilizar o ***sum()*** sem o filtro
"""

dfVendas[(dfVendas.id_cliente == 559) & (dfVendas.id_loja == 2) & (dfVendas.id_produto == 5)] ##Verificando os registros duplicados

"""Outra forma é utilizar um filtro completo para ver todas as duplicatas que foram localizadas.


A menos que haja alguma forma de cruzar os dados da quantidade de produtos comprados pela pessoa com a quantidade que há em estoque no exato momento da compra, é impossivel determinar se foi comprado mais de um produto pela mesma pessoa no mesmo dia.

## Formato de dados

Alterar o formato dos dados.

A função utilizada será o ***pd.to_datetime()*** para alterar a data

A sintaxe da função é:

*pd.todatetime*(**coluna a ser alterada**, *format* = **formato**(**ex:** *%d/%m/y* = *dd-mm-aaaa*)

[documentação](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)
"""

dfClientes.dt_nasc = pd.to_datetime(dfClientes.dt_nasc, format ='%m/%d/%Y') ## alterando de DD/MM/AAAA para AAAA-MM-DD
dfClientes

"""##Indexes"""

dfClientes = dfClientes.set_index('id') ## transforma o id em index
dfLojas = dfLojas.set_index('id')
dfProdutos = dfProdutos.set_index('id')
dfVendas = dfVendas.set_index('id')
dfPag = dfPag.set_index('id')

dfVendas

"""#Data Fusion

Quando for realizar a fusão dos dados, é importante verificar qual é a tabela que possui mais granularidade de informações.

Utilizaremos a função join() para executar esta tarefa.

Sua sintaxe é:

*Tabela principal*.*join*(**tabela que irá complementar as info**, *on* = **'onde as informações irão se mesclar'**)

[documentação](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)
"""

dfVendas.join(dfClientes, on = 'id_cliente') ## Mesclando as tabelas

"""Obsserve que agora a tabela vendas contem as informações de Nome, Sexo e Data de Nascimento"""

df = dfVendas.join(dfClientes.add_prefix('cliente_'), on = 'id_cliente') ## Adicionando um prefixo

"""A função add.prefix() serve para adicionar um prefixo as novas colunas, sendo pussivel identificar de qual tabela aquelas informações estão vindo.


É importante salvar a nova tabela em uma nova váriavel.

Neste caso a variavel é **'df'**
"""

df = df.join(dfLojas.add_prefix('loja_'), on = 'id_loja')
df = df.join(dfProdutos.add_prefix('produto_'), on = 'id_produto')

df

dfClientes.loc[393]

df = df.join(dfPag.set_index('id_venda')) ##Recebendo as datas de pagamento

df

"""NaT = Not a Time ou Não é um registro de tempo."""

df.isnull().sum()

"""#**Criando novos dados (Feature Engineering)**



###***Faça Perguntas que você gostaria de responder!***

Quais Lojas mais vendem?

Quais Produtos mais vendem?

Quais lojas geram maior receita?

Quais Produtos geram maior receita?

Existe um cliente que compre mais?

Existe alguma relação entre loja e cliente?

Qual o tempo médio entre compra e pagamento?
(tempo_pg)

Existe alguma loja em que esse tempo é menor? E produto?

Qual produto gera mais inadimplências?(pg)

Qual loja tem mais inadimplências?

Existe alguma relação entre idade e inadimplência? (cliente_idade)

É possível prever inadimplência através dos dados idade, cidade e produto?

*   Gere novos dados ou transforme!


"""

df['pg'] = 1 ## Gerando uma nova coluna de pagamentos onde 1 = pago e 0 = inadimplente
df.loc[df.dt_pgto.isnull(), 'pg'] = 0 ## Gerando o dado de inadimplencia
df

"""Lembrando que a sintaxe da função ***loc[]*** é:

*df*.*loc*[**linha,nome_coluna**]

[documentação](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)
"""

df['tempo_pg'] = (df.dt_pgto - df.dt_venda).dt.days ## criando a coluna de tempo de pagamento
df

"""o dt.days é utilizado para retirar a string *(days)*, e deixar apenas o número de dias"""

import numpy as np ## importando o numpy
df['cliente_idade'] = np.floor(((pd.to_datetime('today') - df.cliente_dt_nasc).dt.days)/365.25) # criando nova coluna de idade dos clientes
df

"""a função *np.floor()* arredonda para baixo um número float, como pode ser visto nesta [documentação](https://numpy.org/doc/stable/reference/generated/numpy.floor.html)

Já a função np.timedelta64(), é a função que transforma dias em anos, sua sintaxe pode ser conferida nesta [documentação](https://numpy.org/doc/stable/reference/arrays.datetime.html)

# Análises

## Lojas que mais vendem
"""

graf_dados = df.groupby('loja_cidade').count().produto_valor.sort_values(ascending = False) ## Agrupando os dados por cidade, e verificando a quantidade de vendas por cidade
graf_dados

import matplotlib.pyplot as plt ## importando a biblioteca para gerar os gráficos
plt.figure(figsize=(15,5)) ## criando uma figura de 15x5
plt.bar(graf_dados.index, graf_dados.values) ## Criando um grafico de dados, onde o index são as cidades e os valores são a quantidade de vendas.
plt.title('vendas por loja') ## Titulo do gráfico.

"""## Produtos que mais vendem"""

graf_dados = df.groupby('produto_produto').count().produto_valor.sort_values(ascending = False)
graf_dados

plt.figure(figsize=(15,5))
plt.bar(graf_dados.index, graf_dados.values)
plt.title('vendas por produto')

"""Os gráficos são feitos utilizando a biblioteca matplot.
É interessante estudar a [documentação](https://matplotlib.org/stable/index.html) para aprender mais sobre as capacidades dessa biblioteca.

## Receita por loja
"""

graf_dados = df[['loja_cidade', 'produto_valor']].groupby('loja_cidade').sum().produto_valor.sort_values(ascending = False)
graf_dados

plt.figure(figsize=(15,5))
plt.bar(graf_dados.index, graf_dados.values)
plt.title('receita por loja')

print('O maior valor é %i, sendo %.2f vezes a média dos demais, que é %i' % (graf_dados.max(), graf_dados.max()/graf_dados[graf_dados!=graf_dados.max()].mean(), graf_dados[graf_dados!=graf_dados.max()].mean()))

"""Aqui ***(graf_dados.max()/graf_dados[graf_dados!=graf_dados.max()].mean())*** foi realizada uma conta onde pegamos o valor maximo e dividimos pela média.

***!= ***significa diferente

No filtro estamos apenas tirando o valor maximo, que seria do estado do rio e fazendo apenas dos outros estados para sabermos o quantas vezes mais ele é comparado a média dos outros estados

## Receita por produto
"""

graf_dados = df[['produto_produto','produto_valor']].groupby('produto_produto').sum().produto_valor.sort_values(ascending = False)
graf_dados

plt.figure(figsize=(15,5))
plt.bar(graf_dados.index, graf_dados.values)
plt.title('receita por produto')

"""## Subplots"""

plt.figure(figsize = (10,7))
plt.subplot(2,2,1)
graf_dados = df.groupby('loja_cidade').count().produto_valor.sort_values(ascending = False)
plt.bar(graf_dados.index, graf_dados.values)
plt.title('vendas por loja')
plt.xticks(rotation = 90)

plt.subplot(2,2,2)
graf_dados = df.groupby('produto_produto').count().produto_valor.sort_values(ascending = False)
plt.bar(graf_dados.index, graf_dados.values)
plt.title('vendas por produto')
plt.xticks(rotation = 90)

plt.subplot(2,2,3)
graf_dados = df[['loja_cidade', 'produto_valor']].groupby('loja_cidade').sum().produto_valor.sort_values(ascending = False)
plt.bar(graf_dados.index, graf_dados.values)
plt.title('receita por loja')
plt.xticks(rotation = 90)

plt.subplot(2,2,4)
graf_dados = df[['produto_produto','produto_valor']].groupby('produto_produto').sum().produto_valor.sort_values(ascending = False)
plt.bar(graf_dados.index, graf_dados.values)
plt.title('receita por produto')
plt.xticks(rotation = 90)

plt.tight_layout()

"""## Receita por cliente"""

graf_dados = df[['cliente_nome','produto_valor']].groupby('cliente_nome').sum().produto_valor.sort_values(ascending = False)
graf_dados

plt.figure(figsize=(15,5))
plt.plot(graf_dados.index, graf_dados.values)
plt.title('receita por cliente')

from matplotlib.ticker import PercentFormatter

fig,ax = plt.subplots(figsize=(15,5))
ax.plot(graf_dados.index, graf_dados.values, color = 'C0')
ax2 = ax.twinx()
ax2.plot(graf_dados.index, graf_dados.values.cumsum()/graf_dados.values.sum()*100, color = 'C1')
ax2.yaxis.set_major_formatter(PercentFormatter())
ax2.axes.get_xaxis().set_visible(False)
ax.axes.get_xaxis().set_visible(False)
plt.title('Receita por Cliente')

receita_acumulada = graf_dados.cumsum()/graf_dados.sum()
print('O percentual de clientes que compõe 60% da receita é de {:.0f}%'.format(np.floor(receita_acumulada[receita_acumulada<0.60].count()/receita_acumulada.count()*100)))

"""##Pareto"""

graf_dados = df[['produto_produto','produto_valor']].groupby('produto_produto').sum().produto_valor.sort_values(ascending = False)
fig,ax = plt.subplots(figsize=(15,5))
ax.bar(graf_dados.index, graf_dados.values, color = 'C0')
ax2 = ax.twinx()
ax2.plot(graf_dados.index, graf_dados.values.cumsum()/graf_dados.values.sum()*100, color = 'C1', marker = 'D')
ax2.yaxis.set_major_formatter(PercentFormatter())
plt.ylim(0,110)
plt.title('Pareto de Receita por Produto')

"""## Receita combinando produto-loja"""

graf_dados = pd.DataFrame(columns = ('loja','produto','receita')) ## Criando um novo data Frame
temp_list = [] ## Criando uma lista temporária para utilizar o append
for cidade in dfLojas.cidade: ## Fazendo um for no data frame lojas e pegando a cidade em que a loja está localizada
  for produto in dfProdutos.produto: ## Fazendo um for no data frame produtos e pegando o nome do produto
    temp_list.append({ ## puxando os dados para dentro de um objeto
        'loja':cidade,
        'produto': produto,
        'receita': df.produto_valor[(df.loja_cidade == cidade) & (df.produto_produto == produto)].sum()
    })
graf_dados = pd.concat([graf_dados, pd.DataFrame(temp_list)], ignore_index = True) ## Concatenando os dados para o data Frame
graf_dados

import seaborn as sns ## importando a biblioteca seaborn
graf_dados = graf_dados.pivot_table(index = 'loja', columns = 'produto', values = 'receita', aggfunc = 'sum') ##Convertendo os dados em uma tabela pivot/dinâmica
sns.heatmap(graf_dados)

graf_dados = pd.DataFrame(columns = ('loja','produto','receita')) ## Criando um novo data Frame
temp_list = [] ## Criando uma lista temporária para utilizar o append
for cidade in dfLojas.cidade: ## Fazendo um for no data frame lojas e pegando a cidade em que a loja está localizada
  for produto in dfProdutos.produto: ## Fazendo um for no data frame produtos e pegando o nome do produto
    temp_list.append({ ## puxando os dados para dentro de um objeto
        'loja':cidade,
        'produto': produto,
        'receita': df.produto_valor[(df.loja_cidade == cidade) & (df.produto_produto == produto)].sum()
    })
graf_dados = pd.concat([graf_dados, pd.DataFrame(temp_list)], ignore_index = True) ## Concatenando os dados para o data Frame
graf_dados = graf_dados.sort_values('receita', ascending = False)
graf_dados

graf_dados['lojaprod'] = graf_dados.loja + ' - ' + graf_dados.produto
graf_dados

plt.figure(figsize=(15,5))
plt.bar(graf_dados.lojaprod[graf_dados.receita > 0.01*graf_dados.receita.max()], graf_dados.receita[graf_dados.receita > 0.01*graf_dados.receita.max()])
plt.xticks(rotation = 90)
plt.title('receita por loja-produto')
plt.show()

"""É interessante estudar a bibioteca gráfica [Plotly](https://plotly.com/python/)

#Analise de tempo

##Tempo médio de pagamento
"""

df.tempo_pg.mean()

df.boxplot('tempo_pg')
plt.title('Boxplot tempo de pagamento')

sns.histplot(data = df.tempo_pg, kde = True)
plt.title('Distribuição tempo de pagamento')

df.tempo_pg.describe()

"""Com esta analise percebemos que não existe uma previsibilidade no tempo de pagamento

## Tempo de pagamento por cidade e produto
"""

df.groupby('loja_cidade').tempo_pg.mean()

plt.figure(figsize=(7,4))
df[['loja_cidade','tempo_pg']].groupby('loja_cidade').boxplot('tempo_pg')
plt.title('Boxplot tempo de pagamento por cidade')
plt.xticks(rotation = 90)
plt.show()
#

"""Não existe diferença significativa de tempo de pagamento entre as cidades"""

df.groupby('produto_produto').tempo_pg.mean()

plt.figure(figsize=(7,4))
df[['produto_produto','tempo_pg']].groupby('produto_produto').boxplot('tempo_pg')
plt.title('Boxplot tempo de pagamento por produto')
plt.xticks(rotation = 90)
plt.show()
#

"""Também não existe relação entre produto e tempo de pagamento

## Sazonalidade por loja
"""

graf_dados = df[['produto_valor','dt_venda']].groupby('dt_venda').sum().rolling(30).mean()
graf_dados

import plotly.express as px
fig = px.line(x=graf_dados.index, y=graf_dados.values.flatten(), width=700, height=400)
fig.show()

"""é possivel observar uma sazonalidade no meio do ano e no final do ano, mesmo em periodos de poucas vendas

## Vendas por ano
"""

df['Year'] = df['dt_pgto'].dt.year
df

graf_dados = df[['produto_valor','Year']].groupby('Year').sum().produto_valor.sort_values(ascending = False)
graf_dados

graf_dados = df[['produto_valor','Year']].groupby('Year').sum()
plt.figure(figsize=(7,4))
plt.bar(graf_dados.index, graf_dados['produto_valor'])
plt.title('receita por ano')

"""É possivel observar que existe uma queda na receita entre 2019 e 2020

## Tempo de pagamento por promoção
"""

promo = pd.read_csv('caso_estudo_venda_promocao.csv', sep=";")
promo = promo.set_index('id_venda')
promo

df = df.join(promo.add_prefix('promo_'), on = 'id')
df

graf_dados = df.groupby('promo_promoção').tempo_pg.mean()
graf_dados

"""A diferença de tempo médio de pagamento dos que compram o produto fora da promoção é de cerca de 10 dias a menos do que os que compraram na promoção

#Análise de inadimplência

##Inadimplência por loja
"""

plt.figure(figsize=(15,5))

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)
graf_dados = df[df.pg==0].groupby('loja_cidade').count().produto_valor.sort_values(ascending=False)
plt.bar(graf_dados.index,graf_dados.values)
plt.title('Inadimplência por loja')
plt.xticks(rotation=90)

plt.subplot(1,2,2)
graf_dados = df.groupby('loja_cidade').pg.mean().sort_values(ascending=False)
plt.bar(graf_dados.index,graf_dados.values)
plt.title('Percentual de pagamentos por loja')
plt.xticks(rotation=90)

plt.show()

"""##Inadimplência por produto"""

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)
graf_dados = df[df.pg==0].groupby('produto_produto').count().produto_valor.sort_values(ascending=False)
plt.bar(graf_dados.index,graf_dados.values)
plt.title('Inadimplência por produto')
plt.xticks(rotation=90)

plt.subplot( 1,2,2)
graf_dados = df.groupby('produto_produto').pg.mean().sort_values(ascending=False)
plt.bar(graf_dados.index,graf_dados.values)
plt.title('Percentual de pagamentos por produto')
plt.xticks(rotation=90)

plt.show()

"""## Inadimplência por receita"""

graf_dados = df[df.pg == 0].groupby('produto_produto')['produto_valor'].sum().sort_values(ascending=False)
plt.figure(figsize=(15,5))
plt.bar(graf_dados.index,graf_dados.values)
plt.title('Inadimplência por receita')
plt.xticks(rotation=90)
plt.show()

"""## Inadimplência por idade"""

graf_dados = df[df.pg == 0].groupby('cliente_idade').pg.count()
plt.figure(figsize=(15,5))
plt.bar(graf_dados.index,graf_dados.values)
plt.title('Inadimplência por idade')

"""É possivel observar que a inadimplência é maior entre pessoas de 40 e 60 anos

## Mapa de calor de inadimplência
"""

graf_dados = df.pivot_table(index = 'produto_produto', columns = 'loja_cidade', values = 'pg', aggfunc = 'mean')
sns.heatmap(graf_dados, annot=True)

"""# Machine Learning

## Conceitos de ML



*   **Definir um alvo** primeiramente. Podendo ser uma coluna ou uma linha do dataset

*   **Normalizar os dados**. Pois o ML trabalha com dados matematicos

*   Cuidados com **dados Categoricos** como cidades, pois as mesmas não podem ser transferidos em dados númericos, e não representam dados crescentes. Cuidado com Sexo também pois ele pode ser transformados em 0 e 1, porém se não forem dados crescentes eles também são dados categoricos

*   **Dados de treinamentos e dados de testes**, geralmente são divididos em 70 e 80% para treinamento, para que o algoritimo aprenda. Os outros 20 a 30% é usado para testar. Isso serve para prever que o algoritmo fique viciado em dados de treinamento. E perfome bem, mesmo com dados que não foram vistos no treinamento.
O nome desse problema é **Overfitting**.

*   Existem 4 tecnicas que são muito importantes

1. **Organizar os dados**, onde precisamos de um **conjunto de dados e um target**. Digamos que existam 3 colunas de dados e um target. O target é o resultado correto das 3 colunas de dados.
Dessa forma quando tivermos um novo dado, possamos estimar o target dele, com base nos dados de entrada

2. Criar o modelo, que geralmente é **model= modelo(parâmetros)**

3. Entrada dos dados, que seria por exemplo **model.fit(X,Y)** onde **X são os dados de entrada**, e o **Y seria o target ou os dados de saida**.

4. Ao fazer isso basta apenas usar o algoritmo, que geralmente é **model.predict(X)** inserindo apenas os dados de entrada

*   **A matriz de confusão** é usado para avaliar o resultado do modelo de ML. Onde se o valor predito for um **verdadeiro positivo ou um verdadeiro negativo**. O modelo está respondendo bem. Caso seja um **Falso Negativo ou um Falso positivo**, ele está respondendo mal.
Com isso é possivel criar metricas como a **precisão e o recall**. A precisão mostra quantos o algoritmo acertou. O recall mostra o quanto o algoritmo capturou. Eles se antagonizam, pois um mede a precisão, e outro a captura dos dados.

## Alvo
"""

dfML = df[['cliente_sexo','loja_cidade','produto_produto','produto_valor','cliente_idade','promo_promoção','pg']]
dfML = dfML.replace([' ','-'],'_', regex=True)
dfML

"""##Normalização"""

dfML['produto_valor'] = dfML['produto_valor']/dfML['produto_valor'].max()
dfML['cliente_idade'] = dfML['cliente_idade']/dfML['cliente_idade'].max()
dfML

"""## Dados categoricos"""

col_cat = ['cliente_sexo','loja_cidade','produto_produto','promo_promoção']
dfML_dummies = pd.get_dummies(dfML[col_cat].astype(str), drop_first=False, dtype=float)
dfML_dummies

dfML = pd.concat([dfML,dfML_dummies], axis = 1)
dfML = dfML.drop(col_cat, axis = 1)
dfML

from re import X
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, accuracy_score, confusion_matrix, precision_score, recall_score,f1_score, auc

y= dfML.pg
X = dfML.drop('pg', axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_new = X.loc[[2997,2998]]
X_new

"""## Modelos Regressão Linear e Regressão Logistica

Uma **regressão linear** é uma técnica de análise de dados que prevê o valor de dados desconhecidos usando outro valor de dados relacionado e conhecido. Ele modela matematicamente a variável desconhecida ou dependente e a variável conhecida ou independente como uma equação linear. Por exemplo, suponha que você tenha dados sobre suas despesas e receitas do ano passado. As técnicas de regressão linear analisam esses dados e determinam que suas despesas são metade de sua renda. Eles então calculam uma despesa futura desconhecida reduzindo pela metade uma renda futura conhecida. Recomendado para saida de dados continuas.

A regressão logística é uma técnica de análise de dados que usa matemática para encontrar as relações entre dois fatores de dados. Em seguida, essa relação é usada para prever o valor de um desses fatores com base no outro. A previsão geralmente tem um número finito de resultados, como sim ou não.
Por exemplo, digamos que você deseje adivinhar se o visitante do seu site clicará no botão de finalização de compra no carrinho de compras ou não. A análise de regressão logística analisa o comportamento anterior do visitante, como o tempo gasto no site e o número de itens no carrinho. Ela determina que, anteriormente, se os visitantes passassem mais de cinco minutos no site e adicionassem mais de três itens ao carrinho, eles clicavam no botão de finalização de compra. Usando essas informações, a função de regressão logística pode prever o comportamento de um novo visitante do site.
Recomendado quando a saida de dados é binária
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression

all_col_with_plus = '+'.join(dfML.drop(['pg'],axis = 1).columns)
modelo = smf.glm(formula = 'pg ~ ' + all_col_with_plus, data = dfML, family = sm.families.Binomial()).fit()
print(modelo.summary())

model = LogisticRegression(penalty='none', solver = 'newton-cg')
model.fit(X_train, y_train)

print('- Matriz de confusão')
print(confusion_matrix(y_test, model.predict(X_test)))
print('\n Reporte Completo')
print(classification_report(y_test, model.predict(X_test)))
print('\n Reporte teste')
print(classification_report(y_test, model.predict(X_test)))

model.predict(X_new)

"""##Árvore de decisão

Árvores de Decisão são modelos que tentam fazer previsões tomando decisões baseadas em regras que são criadas pelo próprio algoritmo. Essas regras são criadas a partir de features que o modelo julga serem as mais importantes para a tomada de decisão.

Se formos olhar para a estrutura de uma árvore de decisão, ela é composta por 3 elementos: Nós, Arestas e Folhas. Abaixo, vou explicar o que são eles.

**Nós**: Cada nó é uma junção de uma ou mais arestas. Eles são responsáveis por aplicar as regras de decisão.

**Arestas**: São as linhas que conectam um nó a outro.

**Folhas**: São os nós que não têm nenhuma aresta saindo deles. Em Machine Learning, as folhas são os resultados da nossa previsão.
"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn import metrics, tree

model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)
model = model.fit(X_train, y_train)
fig = plt.figure(figsize=(20,10))
_ = tree.plot_tree(model, feature_names=X_train.columns, class_names = ['targetNo', 'targetYes'], filled=True)

print('- Matriz de confusão')
print(confusion_matrix(y_test, model.predict(X_test)))
print('\n Reporte Completo')
print(classification_report(y_test, model.predict(X_test)))
print('\n Reporte teste')
print(classification_report(y_test, model.predict(X_test)))

model.predict(X_new)

"""## Redes Neurais

É uma tecnica de ML, (perception) onde existem alguns dados em neuroniosde dados, e ligações a um função de ativação, e uma saida.
"""

from keras import Sequential
from keras.layers import Dense
import keras
import tensorflow as tf
tf.random.set_seed(2)

model = Sequential()
model.add(Dense(15, activation = 'relu',kernel_initializer = 'random_normal', input_dim = len(X_train.columns)))
model.add(Dense(7, activation = 'relu',kernel_initializer = 'random_normal', input_dim = len(X_train.columns)))
model.add(Dense(3, activation = 'relu',kernel_initializer = 'random_normal', input_dim = len(X_train.columns)))
model.add(Dense(3, activation = 'relu',kernel_initializer = 'random_normal', input_dim = len(X_train.columns)))
model.add(Dense(1, activation = 'sigmoid',kernel_initializer = 'random_normal'))

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train, epochs = 300, verbose = False)

y_pred_probs = model.predict(X_test)
y_pred_classes = (y_pred_probs > 0.5).astype("int32")  # Assuming binary classification

print('- Matriz de confusão')
print(confusion_matrix(y_test, y_pred_classes))
print('\n Reporte Completo')
print(classification_report(y_test, y_pred_classes))
print('\n Reporte teste')
print(classification_report(y_test, y_pred_classes))

model.predict(X_new)

"""##XGBoost

Tecnica que combina outras tecnicas de arvores de decisão. Como se tivessemos vŕias arvore de decisões simples analisando pequenos pontos do código, criando assim um algoritmo composto de sub-algoritmos.
"""

from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)

print('- Matriz de confusão')
print(confusion_matrix(y_test, model.predict(X_test)))
print('\n Reporte Completo')
print(classification_report(y_test, model.predict(X_test)))
print('\n Reporte teste')
print(classification_report(y_test, model.predict(X_test)))

model.predict(X_new)

"""# Framework de trabalho

## Analises

1. Entenda o contexto do problema
2. Extraia os dados
3. Data Cleaning
4. Data Fusion
5. Feature Engineering
6. Gere perguntas
7. Interaja e Visualize (gáficos e tabelas simples que sejam faceis de ler)

Ache informações e tire ideias

##Machine Learning

1. Criar a base com dados relevantes
2. Definir ou criar uma coluna alvo (target)
3. Normalizar os dados
4. Tratar os dados categóricos
5. Separar base treino e teste
6. Testar os modelos de ML

Gerar métricas!
"""

